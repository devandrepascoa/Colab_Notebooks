{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mnist_from_scratch.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN+B6pmrGc68rEIQW2NY7+x"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KkQlHTUeWaTf","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","import pickle as pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mL9bIvXmXSK","colab_type":"code","colab":{}},"source":["class MathUtils():\n","    @staticmethod\n","    def sigmoid(x):\n","        return 1 / (1 + np.exp(-x))\n","\n","    @staticmethod\n","    def sigmoid_deriv(x):\n","        return MathUtils.sigmoid(x) * (1 - MathUtils.sigmoid(x))\n","\n","    @staticmethod\n","    def relu(x):\n","        return np.maximum(0, x)\n","\n","    @staticmethod\n","    def relu_deriv(x):\n","        # return np.greater(x, 0).astype(int)\n","        x[x <= 0] = 0\n","        x[x > 0] = 1\n","        return x\n","\n","    @staticmethod\n","    def softmax(x):\n","        x_exp = np.exp(x)\n","        x_sum = np.sum(x_exp, axis=0, keepdims=True)\n","        return x_exp / x_sum\n","\n","    @staticmethod\n","    def hotOne(array, output_size):\n","        assert len(array.shape) == 2, \"Input has to have shape (data,data_size)\"\n","        Y_orig = array\n","        Y = np.zeros((output_size, Y_orig.shape[-1]))\n","        for i in range(0, Y_orig.shape[1]):\n","            value = Y_orig[0, i]\n","            Y[value, i] = 0.999999999\n","        return Y\n","\n","    @staticmethod\n","    def softmax_deriv(x):\n","        return MathUtils.softmax(x) * (1 - MathUtils.softmax(x))\n","\n","    @staticmethod\n","    def cross_entropy(A, Y):\n","        M = A.shape[1]\n","        logprobs = np.multiply(np.log(A), Y)\n","        cost = - np.sum(logprobs) / M\n","        return float(np.squeeze(cost))\n","\n","    @staticmethod\n","    def cross_entropy_deriv(A, Y):\n","        return -(Y / A) + (1 + Y) / (1 + A)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqN9_ZbFWl3K","colab_type":"code","colab":{}},"source":["def loadMnist():\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","    x_train = (x_train.reshape(x_train.shape[0], -1) / 255.0).T\n","    x_test = (x_test.reshape(x_test.shape[0], -1) / 255.0).T\n","    y_train = (y_train.reshape(y_train.shape[0], 1)).T\n","    y_test = (y_test.reshape(y_test.shape[0], 1)).T\n","    y_train = MathUtils.hotOne(y_train, 10)\n","    y_test = MathUtils.hotOne(y_test, 10)\n","    return (x_train, y_train), (x_test, y_test)\n","\n","\n","def loadCifar10():\n","    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","    x_train = (x_train.reshape(x_train.shape[0], -1) / 255.0).T\n","    x_test = (x_test.reshape(x_test.shape[0], -1) / 255.0).T\n","    y_train = (y_train.reshape(y_train.shape[0], 1)).T\n","    y_test = (y_test.reshape(y_test.shape[0], 1)).T\n","    y_train = MathUtils.hotOne(y_train, 10)\n","    y_test = MathUtils.hotOne(y_test, 10)\n","    print(x_train)\n","    return (x_train, y_train), (x_test, y_test)  \n","\n","(x_train, y_train), (x_test, y_test) = loadMnist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKPROU4ulJTK","colab_type":"code","colab":{}},"source":["class NN():\n","    def init_params(self, S):\n","        parameters = {}\n","        for i in range(1, len(S)):\n","            #(np.sqrt(2 / S[i - 1])) important for weight initialization\n","            parameters[\"W\" + str(i)] = np.random.randn(S[i], S[i - 1]) * (np.sqrt(2 / S[i - 1]))\n","            parameters[\"B\" + str(i)] = np.zeros((S[i], 1))\n","        return parameters\n","\n","    def forward_propagate(self, X, params, S):\n","        cache = {}\n","        cache[\"Z1\"] = np.dot(params[\"W1\"], X) + params[\"B1\"]\n","        cache[\"A1\"] = MathUtils.relu(cache[\"Z1\"])\n","        for i in range(2, len(S) - 1):\n","            cache[\"Z\" + str(i)] = np.dot(params[\"W\" + str(i)],\n","                                         cache[\"A\" + str(i - 1)]) + params[\"B\" + str(i)]\n","            cache[\"A\" + str(i)] = MathUtils.relu(cache[\"Z\" + str(i)])\n","\n","        cache[\"Z\" + str(len(S) - 1)] = np.dot(params[\"W\" + str(len(S) - 1)],\n","                                              cache[\"A\" + str(len(S) - 2)]) + params[\"B\" + str(len(S) - 1)]\n","        cache[\"A\" + str(len(S) - 1)] = MathUtils.softmax(cache[\"Z\" + str(len(S) - 1)])\n","        return cache\n","\n","    def back_propagate(self, X, Y, cache, parameters, S):\n","        gradients = {}\n","        M = Y.shape[1]\n","        gradients[\"dz\" + str(len(S) - 1)] = (cache[\"A\" + str(len(S) - 1)] - Y) / M\n","        for i in range(2, len(S)):\n","            gradients[\"da\" + str(len(S) - i)] = np.dot(parameters[\"W\" + str(len(S) - i + 1)].T,\n","                                                       gradients[\"dz\" + str(len(S) - i + 1)])\n","            gradients[\"dz\" + str(len(S) - i)] = gradients[\"da\" + str(len(S) - i)] * MathUtils.relu_deriv(\n","                cache[\"Z\" + str(len(S) - i)])\n","\n","        gradients[\"dw1\"] = np.dot(gradients[\"dz1\"], X.T)  # dot devido a ser a soma remember my dude produto escalar\n","        gradients[\"db1\"] = np.sum(gradients[\"dz1\"], axis=1, keepdims=True)\n","        for i in range(2, len(S)):\n","            gradients[\"dw\" + str(i)] = np.dot(gradients[\"dz\" + str(i)], cache[\"A\" + str(i - 1)].T)\n","            gradients[\"db\" + str(i)] = np.sum(gradients[\"dz\" + str(i)], axis=1, keepdims=True)\n","\n","        return gradients\n","\n","    def learn(self, gradients, parameters, learning_rate, network_dims):\n","        for i in range(1, len(network_dims)):\n","            parameters[\"W\" + str(i)] -= learning_rate * gradients[\"dw\" + str(i)]\n","            parameters[\"B\" + str(i)] -= learning_rate * gradients[\"db\" + str(i)]\n","\n","    def get_accuracy(self, Yhat, Y):\n","        m = Yhat.shape[1]\n","        sum = 0\n","        Yhat_max = np.argmax(Yhat, axis=0)\n","        Y_max = np.argmax(Y, axis=0)\n","        for i in range(0, m):\n","            if Yhat_max[i] == Y_max[i]:\n","                sum += 1\n","        return (sum / m) * 100.0\n","\n","    def evaluate(self, dataset):\n","        (X, Y) = dataset\n","        cache = self.forward_propagate(X, self.params, self.S)\n","        cost = MathUtils.cross_entropy(cache[\"A\" + str(len(self.S) - 1)], Y)\n","        accuracy = self.get_accuracy(cache[\"A\" + str(len(self.S) - 1)], Y)\n","        return {\"cache\": cache, \"cost\": cost, \"accuracy\": accuracy}\n","\n","    # (1,784) <--input data shape\n","    def predict(self, input_data):\n","        assert input_data.shape == (self.S[0], 1)\n","        X = input_data\n","\n","        cache = self.forward_propagate(X, self.params, self.S)\n","        label = np.argmax(cache[\"A\" + str(len(self.S) - 1)])\n","        return label\n","\n","    def validate_dataset(self, dataset, name):\n","        assert isinstance(dataset, tuple) and len(dataset) == 2, (\n","                name + \" has to be a tuple of size 2 -> (x_train,y_train)\")\n","        assert isinstance(dataset[0], np.ndarray) and isinstance(dataset[1], np.ndarray), (\n","                name + \" data has to be numpy array\")\n","        assert len(dataset[0].shape) == 2 and len(dataset[1].shape) == 2, (\n","                name + \" data has to be of shape (data_size,data) and labels (label_size,label)\")\n","\n","    def save(self, path=\"neural_network\"):\n","        outfile = open(path, 'wb')\n","        pickle.dump(self, outfile)\n","        outfile.close()\n","\n","    @staticmethod\n","    def load(path=\"neural_network\"):\n","        infile = open(path, 'rb')\n","        nn = pickle.load(infile)\n","        infile.close()\n","        return nn\n","\n","    def __init__(self, dataset, val_dataset=None, epochs=2000, learning_rate=0.5, shape=None,\n","                 print_costs=True,\n","                 stop_when_loss=True):\n","        # Validations\n","        self.validate_dataset(dataset, \"Data set\")\n","        if val_dataset is not None:\n","            self.validate_dataset(val_dataset, \"Validation set\")\n","\n","        (X, Y) = dataset\n","        assert len(X.shape) == 2\n","        assert len(Y.shape) == 2\n","        if val_dataset is not None:\n","            (X_test, Y_test) = val_dataset\n","            assert len(X_test.shape) == 2\n","            assert len(Y_test.shape) == 2\n","\n","        if shape is not None:\n","            self.S = shape\n","            assert len(self.S) > 2\n","            assert self.S[0] == X.shape[0] and self.S[-1] == Y.shape[0]\n","        else:\n","            self.S = [dataset[0].shape[0], 30, 10]\n","        self.params = self.init_params(self.S)\n","        \n","        previous_accuracy = 0\n","        previous_val_accuracy = 0\n","        for i in range(0, epochs):\n","            cache = self.forward_propagate(X, self.params, self.S)\n","            cost = MathUtils.cross_entropy(cache[\"A\" + str(len(self.S) - 1)], Y)\n","            accuracy = self.get_accuracy(cache[\"A\" + str(len(self.S) - 1)], Y)\n","            grads = self.back_propagate(X, Y, cache, self.params, self.S)\n","            self.learn(grads, self.params, learning_rate, self.S)\n","\n","            if i % 100 == 0:\n","                if print_costs:\n","                    print(\"Epoch:{},Cost:{}, Accuracy:{}\".format(i, cost, accuracy))\n","                    if val_dataset is not None:\n","                        cache_test = self.forward_propagate(X_test, self.params, self.S)\n","                        cost_test = MathUtils.cross_entropy(cache_test[\"A\" + str(len(self.S) - 1)], Y_test)\n","                        accuracy_test = self.get_accuracy(cache_test[\"A\" + str(len(self.S) - 1)], Y_test)\n","                        print(\"Validation Cost:{}, Validation Accuracy:{}\".format(cost_test, accuracy_test))\n","            if stop_when_loss:\n","                previous_accuracy = accuracy\n","                if val_dataset is not None:\n","                    previous_val_accuracy = accuracy_test\n","                    if previous_val_accuracy > accuracy_test:\n","                        break\n","                else:\n","                    if previous_accuracy > accuracy:\n","                        break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhuz93wOfUF_","colab_type":"code","outputId":"c696ae46-235e-4f2b-c787-3d720a35cf11","colab":{"base_uri":"https://localhost:8080/","height":731},"executionInfo":{"status":"error","timestamp":1586786831226,"user_tz":-60,"elapsed":45131,"user":{"displayName":"André Páscoa","photoUrl":"","userId":"05604818833942380657"}}},"source":["nn = NN((x_train,y_train),val_dataset=(x_test,y_test), shape=[784,200,80, 10],epochs = 10000,learning_rate=0.01)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Shape A1[0]: 200 , Shape A1[1]:60000\n","Epoch:0,Cost:2.428848745603354, Accuracy:9.415\n","Validation Cost:2.4077156537790256, Validation Accuracy:10.09\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n","Shape A1[0]: 200 , Shape A1[1]:60000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d0cc36010dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-365b94297646>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, val_dataset, epochs, learning_rate, shape, print_costs, stop_when_loss)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMathUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-365b94297646>\u001b[0m in \u001b[0;36mback_propagate\u001b[0;34m(self, X, Y, cache, parameters, S)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 cache[\"Z\" + str(len(S) - i)])\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dw1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dz1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dot devido a ser a soma remember my dude produto escalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dz1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"lx2r7XNmnhfQ","colab_type":"code","colab":{}},"source":["filename = 'neural_network'\n","outfile = open(filename,'wb')\n","pickle.dump(nn,outfile)\n","outfile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMuJ1lMRnpDJ","colab_type":"code","colab":{}},"source":["infile = open('neural_network', 'rb')\n","nn = pickle.load(infile)\n","infile.close()\n","\n","index = 49\n","d = new_nn.predict(x_test[index].reshape(1, 784))\n","\n","print(\"Predicted {}, Result:{}\".format(d, y_test[index]))\n","plt.imshow(x_test[index].reshape(28,28))\n","plt.show()"],"execution_count":0,"outputs":[]}]}